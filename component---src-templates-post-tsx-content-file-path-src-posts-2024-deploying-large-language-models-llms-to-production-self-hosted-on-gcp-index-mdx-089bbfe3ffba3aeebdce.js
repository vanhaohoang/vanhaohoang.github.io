(self.webpackChunkvanhaohoang_github_io=self.webpackChunkvanhaohoang_github_io||[]).push([[551],{3150:function(e,a,n){var t=n(9720).w_;e.exports.C=function(e){return t({tag:"svg",attr:{viewBox:"0 0 24 24"},child:[{tag:"path",attr:{d:"M11.953,2C6.465,2,2,6.486,2,12s4.486,10,10,10s10-4.486,10-10S17.493,2,11.953,2z M12,20c-4.411,0-8-3.589-8-8 s3.567-8,7.953-8C16.391,4,20,7.589,20,12S16.411,20,12,20z"}},{tag:"path",attr:{d:"M11 7H13V14H11zM11 15H13V17H11z"}}]})(e)}},9881:function(e,a,n){var t=n(9720).w_;e.exports.L=function(e){return t({tag:"svg",attr:{viewBox:"0 0 16 16",fill:"currentColor"},child:[{tag:"path",attr:{fillRule:"evenodd",d:"M8 16A8 8 0 108 0a8 8 0 000 16zm.93-9.412l-2.29.287-.082.38.45.083c.294.07.352.176.288.469l-.738 3.468c-.194.897.105 1.319.808 1.319.545 0 1.178-.252 1.465-.598l.088-.416c-.2.176-.492.246-.686.246-.275 0-.375-.193-.304-.533L8.93 6.588zM8 5.5a1 1 0 100-2 1 1 0 000 2z",clipRule:"evenodd"}}]})(e)}},4363:function(e,a,n){var t=n(9720).w_;e.exports.v=function(e){return t({tag:"svg",attr:{viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round"},child:[{tag:"rect",attr:{x:"3",y:"4",width:"18",height:"18",rx:"2",ry:"2"}},{tag:"line",attr:{x1:"16",y1:"2",x2:"16",y2:"6"}},{tag:"line",attr:{x1:"8",y1:"2",x2:"8",y2:"6"}},{tag:"line",attr:{x1:"3",y1:"10",x2:"21",y2:"10"}}]})(e)}},550:function(e,a,n){var t=n(9720).w_;e.exports.T=function(e){return t({tag:"svg",attr:{viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round"},child:[{tag:"circle",attr:{cx:"12",cy:"12",r:"10"}},{tag:"polyline",attr:{points:"12 6 12 12 16 14"}}]})(e)}},9876:function(e,a,n){"use strict";n.r(a),n.d(a,{default:function(){return i}});var t=n(1151),l=n(7294);function s(e){const a=Object.assign({h2:"h2",a:"a",span:"span",p:"p",h3:"h3",ul:"ul",li:"li",blockquote:"blockquote",strong:"strong",h4:"h4"},(0,t.ah)(),e.components);return l.createElement(l.Fragment,null,l.createElement(a.h2,{id:"objective",style:{position:"relative"}},"Objective",l.createElement(a.a,{href:"#objective","aria-label":"objective permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.p,null,"This document outlines the various open-source libraries, tools, and best practices for deploying Large Language Models (LLMs) in a self-hosted production environment on Google Cloud Platform (GCP). The primary focus is on ensuring high security by avoiding the use of third-party services."),"\n",l.createElement(a.h2,{id:"compare-open-source-tools-for-model-serving",style:{position:"relative"}},"Compare Open-Source Tools for Model Serving",l.createElement(a.a,{href:"#compare-open-source-tools-for-model-serving","aria-label":"compare open source tools for model serving permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement("table",{style:{borderCollapse:"collapse",width:"100%"}},l.createElement("thead",null,l.createElement("tr",null,l.createElement("th",{style:{border:"1px solid black",padding:"8px"}},"Tool"),l.createElement("th",{style:{border:"1px solid black",padding:"8px"}},"Focus"),l.createElement("th",{style:{border:"1px solid black",padding:"8px"}},"Model Support"),l.createElement("th",{style:{border:"1px solid black",padding:"8px"}},"Usage"))),l.createElement("tbody",null,l.createElement("tr",null,l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"vLLM"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"GPU"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Limited Models"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"GPU Server")),l.createElement("tr",null,l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Llama.cpp"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"CPU"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Any Models"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"CPU Server")),l.createElement("tr",null,l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Llamafile"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"CPU"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Any Models"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"CPU Server")),l.createElement("tr",null,l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Ollama"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"CPU"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Any Models"),l.createElement("td",{style:{border:"1px solid black",padding:"8px"}},"Developer (Build/Test)")))),"\n",l.createElement(a.h2,{id:"deploy-llms-model-using-vllm",style:{position:"relative"}},"Deploy LLMs Model using vLLM",l.createElement(a.a,{href:"#deploy-llms-model-using-vllm","aria-label":"deploy llms model using vllm permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.h3,{id:"system-requirements",style:{position:"relative"}},"System Requirements",l.createElement(a.a,{href:"#system-requirements","aria-label":"system requirements permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.ul,null,"\n",l.createElement(a.li,null,"GPU accelerator for fast inference latency"),"\n",l.createElement(a.li,null,"GPU with large memory (e.g., NVIDIA A100)"),"\n",l.createElement(a.li,null,"Use a system with resources matching the model's fine-tuning requirements"),"\n",l.createElement(a.li,null,"Using vLLM (version ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">0.6.0</code>'}}),"), providing an ",l.createElement(a.a,{href:"https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"},"OpenAI compatible server")),"\n"),"\n",l.createElement(a.h3,{id:"deploy-the-fine-tuned-model-to-a-development-or-local-environment",style:{position:"relative"}},"Deploy the Fine-Tuned Model to a Development or Local Environment",l.createElement(a.a,{href:"#deploy-the-fine-tuned-model-to-a-development-or-local-environment","aria-label":"deploy the fine tuned model to a development or local environment permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.p,null,"Assuming you have:"),"\n",l.createElement(a.ul,null,"\n",l.createElement(a.li,null,l.createElement(a.a,{href:"https://docs.vllm.ai/en/latest/getting_started/installation.html"},"vLLM==0.6.0")),"\n",l.createElement(a.li,null,"Fine-tuned model files in a directory called ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">finetuned_model</code>'}})),"\n"),"\n",l.createElement(a.p,null,"You can deploy it locally using the following command:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash">vllm serve finetuned_model</code></pre></div>'}}),"\n",l.createElement(a.blockquote,null,"\n",l.createElement(a.p,null,l.createElement(a.strong,null,"Note:")),"\n",l.createElement(a.ul,null,"\n",l.createElement(a.li,null,"The default model name for API calls will be ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">finetuned_model</code>'}})," unless overridden with flags."),"\n",l.createElement(a.li,null,"Access the vLLM server at ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">localhost</code>'}}),"."),"\n",l.createElement(a.li,null,"Refer to the ",l.createElement(a.a,{href:"https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server"},"official documentation")," for available flags."),"\n"),"\n"),"\n",l.createElement(a.h3,{id:"deploy-the-fine-tuned-model-to-a-production-environment",style:{position:"relative"}},"Deploy the Fine-Tuned Model to a Production Environment",l.createElement(a.a,{href:"#deploy-the-fine-tuned-model-to-a-production-environment","aria-label":"deploy the fine tuned model to a production environment permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.h4,{id:"vm-instance",style:{position:"relative"}},"VM Instance",l.createElement(a.a,{href:"#vm-instance","aria-label":"vm instance permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.ul,null,"\n",l.createElement(a.li,null,"Use the ",l.createElement(a.a,{href:"https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html"},"official vLLM Docker image")," for running a fine-tuned model on a VM instance (vLLM ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">0.6.0</code>'}}),")."),"\n",l.createElement(a.li,null,"Prerequisites:","\n",l.createElement(a.ul,null,"\n",l.createElement(a.li,null,l.createElement(a.a,{href:"https://docs.docker.com/engine/install/"},"Docker")),"\n",l.createElement(a.li,null,l.createElement(a.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"},"NVIDIA Container Toolkit")),"\n",l.createElement(a.li,null,"Fine-tuned model files in ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">finetuned_model</code>'}})),"\n"),"\n"),"\n"),"\n",l.createElement(a.p,null,"Run the following Docker command, updating the ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">--served-model-name</code>'}})," flag to override the model name:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="docker"><pre class="language-docker"><code class="language-docker">docker run --runtime nvidia --gpus all \\\n  -v "./finetuned_model:/mnt/models" \\\n  -p 8000:8000 --ipc="host" \\\n  vllm/vllm-openai:latest \\\n  --model "/mnt/models" \\\n  --served-model-name "CHANGEME"</code></pre></div>'}}),"\n",l.createElement(a.h4,{id:"kubernetes-cluster",style:{position:"relative"}},"Kubernetes Cluster",l.createElement(a.a,{href:"#kubernetes-cluster","aria-label":"kubernetes cluster permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.p,null,"We recommend using ",l.createElement(a.a,{href:"https://github.com/kserve/kserve"},"KServe")," when deploying a fine-tuned model to a ",l.createElement(a.a,{href:"https://kubernetes.io/"},"Kubernetes")," cluster, due to the ",l.createElement(a.a,{href:"https://kserve.github.io/website/latest/modelserving/v1beta1/llm/huggingface/"},"integrated vLLM runtime"),"."),"\n",l.createElement(a.p,null,"Ensure you have:"),"\n",l.createElement(a.ul,null,"\n",l.createElement(a.li,null,"A Kubernetes cluster with ",l.createElement(a.a,{href:"https://kserve.github.io/website/latest/admin/serverless/serverless/"},"KServe installed")),"\n",l.createElement(a.li,null,"At least one ",l.createElement(a.a,{href:"https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/"},"NVIDIA GPU node with appropriate drivers")),"\n"),"\n",l.createElement(a.p,null,"Use the following manifest to deploy the model from a cloud storage bucket:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="yaml"><pre class="language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> serving.kserve.io/v1beta1\n<span class="token key atrule">kind</span><span class="token punctuation">:</span> InferenceService\n<span class="token key atrule">metadata</span><span class="token punctuation">:</span>\n  <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">"CHANGEME"</span>\n<span class="token key atrule">spec</span><span class="token punctuation">:</span>\n  <span class="token key atrule">predictor</span><span class="token punctuation">:</span>\n    <span class="token key atrule">containers</span><span class="token punctuation">:</span>\n    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">"main"</span>\n      <span class="token key atrule">image</span><span class="token punctuation">:</span> <span class="token string">"kserve/vllmserver:latest"</span>\n      <span class="token key atrule">command</span><span class="token punctuation">:</span>\n      <span class="token punctuation">-</span> <span class="token string">"python3"</span>\n      <span class="token punctuation">-</span> <span class="token string">"-m"</span>\n      <span class="token punctuation">-</span> <span class="token string">"vllm.entrypoints.openai.api_server"</span>\n      <span class="token key atrule">args</span><span class="token punctuation">:</span>\n      <span class="token punctuation">-</span> <span class="token string">"--port"</span>\n      <span class="token punctuation">-</span> <span class="token string">"8000"</span>\n      <span class="token punctuation">-</span> <span class="token string">"--model"</span>\n      <span class="token punctuation">-</span> <span class="token string">"/mnt/models"</span>\n      <span class="token punctuation">-</span> <span class="token string">"--served-model-name"</span>\n      <span class="token punctuation">-</span> <span class="token string">"CHANGEME"</span>\n      <span class="token key atrule">env</span><span class="token punctuation">:</span>\n      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">"STORAGE_URI"</span>\n        <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"CHANGEME"</span>\n      <span class="token key atrule">resources</span><span class="token punctuation">:</span>\n        <span class="token key atrule">limits</span><span class="token punctuation">:</span>\n          <span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation">:</span> <span class="token string">"1"</span></code></pre></div>'}}),"\n",l.createElement(a.p,null,"Make sure to update the values for:"),"\n",l.createElement(a.ul,null,"\n",l.createElement(a.li,null,l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">metadata.name</code>'}})," for your model inference service"),"\n",l.createElement(a.li,null,l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">STORAGE_URI</code>'}})," for your cloud storage URI"),"\n",l.createElement(a.li,null,l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">--served-model-name</code>'}})," for the model name used in API calls"),"\n"),"\n",l.createElement(a.h2,{id:"deploy-llms-model-using-ollama",style:{position:"relative"}},"Deploy LLMs Model using Ollama",l.createElement(a.a,{href:"#deploy-llms-model-using-ollama","aria-label":"deploy llms model using ollama permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.h3,{id:"deploy-the-fine-tuned-model-to-a-development-or-local-environment-1",style:{position:"relative"}},"Deploy the Fine-Tuned Model to a Development or Local Environment",l.createElement(a.a,{href:"#deploy-the-fine-tuned-model-to-a-development-or-local-environment-1","aria-label":"deploy the fine tuned model to a development or local environment 1 permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.h3,{id:"deploy-the-fine-tuned-model-to-a-production-environment-1",style:{position:"relative"}},"Deploy the Fine-Tuned Model to a Production Environment",l.createElement(a.a,{href:"#deploy-the-fine-tuned-model-to-a-production-environment-1","aria-label":"deploy the fine tuned model to a production environment 1 permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.p,null,"Use the following Dockerfile to deploy Ollama to production:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="docker"><pre class="language-docker"><code class="language-docker"><span class="token instruction"><span class="token keyword">FROM</span> ollama/ollama:0.3.6</span>\n\n<span class="token instruction"><span class="token keyword">ENV</span> OLLAMA_HOST 0.0.0.0:8080</span>\n<span class="token instruction"><span class="token keyword">ENV</span> OLLAMA_MODELS /models</span>\n<span class="token instruction"><span class="token keyword">ENV</span> OLLAMA_DEBUG false</span>\n<span class="token instruction"><span class="token keyword">ENV</span> OLLAMA_KEEP_ALIVE -1</span>\n<span class="token instruction"><span class="token keyword">ENV</span> MODEL gemma2:9b</span>\n\n<span class="token instruction"><span class="token keyword">RUN</span> ollama serve &amp; sleep 5 &amp;&amp; ollama pull <span class="token variable">$MODEL</span> </span>\n\n<span class="token instruction"><span class="token keyword">ENTRYPOINT</span> [<span class="token string">"ollama"</span>, <span class="token string">"serve"</span>]</span></code></pre></div>'}}),"\n",l.createElement(a.p,null,"Create an artifact repository on GCP:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash">gcloud artifacts repositories create ollama <span class="token punctuation">\\</span>\n  --repository-format<span class="token operator">=</span>docker <span class="token punctuation">\\</span>\n  <span class="token parameter variable">--location</span><span class="token operator">=</span>us-central1</code></pre></div>'}}),"\n",l.createElement(a.p,null,"Build the image and submit it:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash">gcloud builds submit <span class="token punctuation">\\</span>\n  <span class="token parameter variable">--tag</span> us-central1-docker.pkg.dev/<span class="token punctuation">[</span>PROJECT-NAME<span class="token punctuation">]</span>/ollama/ollama-gemma <span class="token punctuation">\\</span>\n  --machine-type e2-highcpu-32</code></pre></div>'}}),"\n",l.createElement(a.p,null,"Create a service account:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash">gcloud iam service-accounts create ollama <span class="token punctuation">\\</span>\n  --display-name<span class="token operator">=</span><span class="token string">"Service Account for Ollama Cloud Run service"</span></code></pre></div>'}}),"\n",l.createElement(a.p,null,"Activate the model:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash">gcloud beta run deploy ollama-gemma <span class="token punctuation">\\</span>\n  <span class="token parameter variable">--image</span> us-central1-docker.pkg.dev/<span class="token punctuation">[</span>PROJECT-NAME<span class="token punctuation">]</span>/ollama/ollama-gemma <span class="token punctuation">\\</span>\n  <span class="token parameter variable">--concurrency</span> <span class="token number">4</span> <span class="token punctuation">\\</span>\n  <span class="token parameter variable">--cpu</span> <span class="token number">8</span> <span class="token punctuation">\\</span>\n  --set-env-vars <span class="token assign-left variable">OLLAMA_NUM_PARALLEL</span><span class="token operator">=</span><span class="token number">4</span> <span class="token punctuation">\\</span>\n  --max-instances <span class="token number">2</span> <span class="token punctuation">\\</span>\n  <span class="token parameter variable">--memory</span> 32Gi <span class="token punctuation">\\</span>\n  --no-allow-unauthenticated <span class="token punctuation">\\</span>\n  --no-cpu-throttling <span class="token punctuation">\\</span>\n  --service-account ollama@<span class="token punctuation">[</span>PROJECT-NAME<span class="token punctuation">]</span>.iam.gserviceaccount.com <span class="token punctuation">\\</span>\n  <span class="token parameter variable">--timeout</span><span class="token operator">=</span><span class="token number">600</span></code></pre></div>'}}),"\n",l.createElement(a.p,null,"The model can now be accessed using ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">curl</code>'}}),":"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-H</span> <span class="token string">"Authorization: Bearer <span class="token variable"><span class="token variable">$(</span>gcloud auth print-identity-token<span class="token variable">)</span></span>"</span> <span class="token punctuation">\\</span>\n     <span class="token parameter variable">-H</span> <span class="token string">"Content-Type: application/json"</span> <span class="token punctuation">\\</span>\n     https://ollama-gemma-wruklsvs3a-uc.a.run.app/api/generate <span class="token parameter variable">-d</span> <span class="token string">\'{\n       "model": "gemma2:9b",\n       "prompt": "Why is the sky blue?",\n       "stream": false\n     }\'</span></code></pre></div>'}}),"\n",l.createElement(a.h2,{id:"deploy-llms-model-using-llamafile",style:{position:"relative"}},"Deploy LLMs Model using Llamafile",l.createElement(a.a,{href:"#deploy-llms-model-using-llamafile","aria-label":"deploy llms model using llamafile permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.h3,{id:"step-1-download-the-llamafile",style:{position:"relative"}},"Step 1: Download the Llamafile",l.createElement(a.a,{href:"#step-1-download-the-llamafile","aria-label":"step 1 download the llamafile permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.p,null,"Download the ",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">llava-v1.5-7b-q4.llamafile</code>'}})," (3.97 GB) executable from ",l.createElement(a.a,{href:"https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true"},"here"),"."),"\n",l.createElement(a.h3,{id:"step-2-grant-execution-permission",style:{position:"relative"}},"Step 2: Grant Execution Permission",l.createElement(a.a,{href:"#step-2-grant-execution-permission","aria-label":"step 2 grant execution permission permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.p,null,"For macOS, Linux, or BSD users:"),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash"><span class="token function">chmod</span> +x llava-v1.5-7b-q4.llamafile</code></pre></div>'}}),"\n",l.createElement(a.h3,{id:"step-3-run-the-llamafile",style:{position:"relative"}},"Step 3: Run the Llamafile",l.createElement(a.a,{href:"#step-3-run-the-llamafile","aria-label":"step 3 run the llamafile permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="bash"><pre class="language-bash"><code class="language-bash">./llava-v1.5-7b-q4.llamafile <span class="token parameter variable">-ngl</span> <span class="token number">9999</span></code></pre></div>'}}),"\n",l.createElement(a.h3,{id:"step-4-interact-with-the-ui",style:{position:"relative"}},"Step 4: Interact with the UI",l.createElement(a.a,{href:"#step-4-interact-with-the-ui","aria-label":"step 4 interact with the ui permalink",className:"gatsby-remark-autolink-header-anchor after"},l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}}))),"\n",l.createElement(a.p,null,"After running, the Llamafile will open the user interface at ",l.createElement(a.a,{href:"http://localhost:8080/"},"http://localhost:8080"),"."),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 936px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 133.6%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAbCAIAAADzvTiPAAAACXBIWXMAAAsTAAALEwEAmpwYAAADyklEQVR42m1U13LkNhDUH9gnLnMCQRDMOS13Ka61StZJZ1948B/4C/z/5WbQykFVTRYwmMYMGpi5khT1PxBlRdH0tt83bVc3bd10O0n+vxtwhU9WNQAERTNU3cBA002Q++E4HG+G44jV1b56AhsZcSybBmGapGXbDXWzb7uD64W6bgkGs1ksKyrzgzjNLOIIogR/YOVfISVCWVl11OWEepT5GNiOazsMgAUwbMdxPYKpw0ybGiZBIhvZZf7z89fT6fHl9cfLy/fPn7/9/vWP1y8Yf3t4eP388v3x8cvT02/rfxzvoqjCFqIsz2TsOp3u9sOYF01RtVkBjYYsr/OiHg7TcJzqpmdeQBwE95COQ7lmWEj+nZxmlWE6pkUBQZR/FoRPwq5ue+wCu0VcMHXTFiRxJ0mbYOuZD+Op3x8RwSLUJq5pEfjpho0lnB/GxU5N29ENC4DyF7J3HO+m6T5JasZiz0s8L53BU0zZPH0HY6nv56blvKVN+f39y+35KQhy6kacp+/e2zj9BzkONrKMe5aJ4+KqoFNeNmmOk5P15B9hFsWyXbwiUVFmMgS8uTl3/dh2x+l0D80pDVw3nME+gMdjwyJz2gvZxdviQQydoDymEAaAxzvM9wHctkeyRoZUUVQGYR5GJQZxXOHPfWiTAXxG+vZP4LYJtrxtly+SrK4Qc9Es5TxjLFn0x3gD3IKwQF7bC6Ou17aHouzjpAKw8X4/HcfzNN09PT5XZatqpqKZqm7i5nH/xiVtCVVj0vZ43/V7x2WU4cxUM8ylDE2W7ofpIU5Tm1LY12pfqmqrZ0XUCE/7qsjjJE2yPIwT0JYGoP1kZSzpy7KCPUpS3Zyf9NoCNsFM4vL84OXHpjt0/eD5weohqXqF8m67qunKqinrBvv+i4zIkk4Ep752mrw+JGnGgxBp4ziCrAVZV1dVEEVhkqIfEOp+FDnbowyrqrKpCzFWD1UznLBGRnU3uB5Hzpfu9RZZlhWT7vhYHx76/ZCXdRglMopGVWXNEFmrR9Mw3pZVnealYdnizEQz07cGiBh5GhXFLMlOFOftwFRVBUuSIEvXwm53vduJcxXPdnRX5aI2Iqs212Z4qsUU05UNiv8CplgMRtVmlyWT+qpFtxdmZ2e/fODF2ctuaTw58U1YP7jJiS4g0Y0djlZwtIIDxibrf/3zr+T84/rT9RxZtn3ViVQSviEw3ES1Q9UOZJNHxSFrpnp/141PcTkSXg6/vIbFAce4Wh6NdAH6k2ag/RCJ+AJJBDt2/dRJBxqWnHPiUFXTCfqRaSPlvwHfmueqYBNtUgAAAABJRU5ErkJggg==\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="Llamafile UI"\n        title=""\n        src="/static/6b6e71b4afebb36de5b15a708bf98b70/6d2da/llamafile-ui.png"\n        srcset="/static/6b6e71b4afebb36de5b15a708bf98b70/63868/llamafile-ui.png 250w,\n/static/6b6e71b4afebb36de5b15a708bf98b70/0b533/llamafile-ui.png 500w,\n/static/6b6e71b4afebb36de5b15a708bf98b70/6d2da/llamafile-ui.png 936w"\n        sizes="(max-width: 936px) 100vw, 936px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}}))}var r=function(e){void 0===e&&(e={});const{wrapper:a}=Object.assign({},(0,t.ah)(),e.components);return a?l.createElement(a,e,l.createElement(s,e)):s(e)},o=n(7084);const c=e=>{const{data:a,children:n}=e;return l.createElement(o.Z,{post:a},n)};function i(e){return l.createElement(c,e,l.createElement(r,e))}},7084:function(e,a,n){"use strict";n.d(a,{Z:function(){return h}});var t=n(7294),l=n(578),s=n(4363),r=n(550),o=n(9737),c=n(5103),i=n(2717),p=n(8288);var d=e=>{var a,n,l,d,m,u,h;const{post:g,children:v}=e,k=null!=g&&null!==(a=g.mdx)&&void 0!==a&&null!==(n=a.frontmatter)&&void 0!==n&&n.date?t.createElement(c.Z,{className:"mr-6 mb-6 text-gray-500 text-sm"},t.createElement(s.v,{className:"mr-1"}),null==g||null===(l=g.mdx)||void 0===l?void 0:l.frontmatter.date):null,f=null!=g&&null!==(d=g.mdx)&&void 0!==d&&d.timeToRead?t.createElement(c.Z,null,t.createElement(r.T,{className:"mr-1"}),(0,i.j)(null==g||null===(m=g.mdx)||void 0===m?void 0:m.timeToRead)," min to read"):null;return t.createElement("div",{className:"flex flex-col items-center"},t.createElement("article",{className:"w-full prose prose-sm sm:prose overflow-hidden prose-red",style:{maxWidth:"860px"}},t.createElement(o.Z,null,(null===(u=g.mdx)||void 0===u||null===(h=u.frontmatter)||void 0===h?void 0:h.title)||""),t.createElement(c.Z,null,k,f),t.createElement(p.Z,null,v)))},m=n(7943),u=n(5677);var h=e=>{var a,n,s,r,o,c,i,h,g,v,k;const{post:f,children:b}=e;return t.createElement(l.Z,null,t.createElement(m.ZP,{title:(null===(a=f.mdx)||void 0===a||null===(n=a.frontmatter)||void 0===n?void 0:n.title)||"",titleMode:m.H0,description:(null===(s=f.mdx)||void 0===s||null===(r=s.frontmatter)||void 0===r?void 0:r.summary)||"",image:(null===(o=f.mdx)||void 0===o||null===(c=o.frontmatter)||void 0===c||null===(i=c.cover)||void 0===i||null===(h=i.childImageSharp)||void 0===h||null===(g=h.gatsbyImageData)||void 0===g||null===(v=g.images)||void 0===v||null===(k=v.fallback)||void 0===k?void 0:k.src)||"",type:m.Sp}),t.createElement(d,{post:f},b),t.createElement("div",{className:"flex flex-row justify-center items-center mt-16"},t.createElement("div",{className:"max-w-md"},t.createElement(p.Z,null,t.createElement(u.Z,null)))))}},2091:function(e,a,n){"use strict";n.d(a,{hW:function(){return c}});var t=n(7294),l=n(9881),s=n(3150),r=n(5103);const o="info",c="error",i={[o]:t.createElement(l.L,{size:18}),[c]:t.createElement(s.C,{size:18})},p={[o]:"text-blue-600 bg-blue-100",[c]:"text-red-600 bg-red-100"};a.ZP=e=>{const{children:a,type:n}=e;return a?t.createElement("div",{className:"py-3 px-4 rounded-md "+p[n]},t.createElement(r.Z,null,t.createElement("div",{className:"mr-3"},i[n]),t.createElement("div",{className:"text-sm"},a))):null}},8288:function(e,a,n){"use strict";var t=n(4578),l=n(7294),s=n(2091);let r=function(e){function a(a){var n;return(n=e.call(this,a)||this).state={hasError:!1},n}(0,t.Z)(a,e),a.getDerivedStateFromError=function(){return{hasError:!0}};var n=a.prototype;return n.componentDidCatch=function(e,a){console.error(e,a),this.setState({hasError:!0})},n.render=function(){const{children:e}=this.props,{hasError:a}=this.state;return a?l.createElement(s.ZP,{type:s.hW},"Component has crashed"):e},a}(l.Component);a.Z=r},9737:function(e,a,n){"use strict";var t=n(7294),l=n(4519);a.Z=e=>{const{children:a,className:n=""}=e,s="mb-6 uppercase font-extrabold "+n;return t.createElement(l.Z,{level:l._.h1,className:s},a)}},5677:function(e,a,n){"use strict";var t=n(7294);a.Z=e=>{const{withHeader:a=!0}=e,n="border py-2 px-3 mb-3 rounded border-gray-300 border-solid appearance-none",l=a?t.createElement("h1",{className:"text-grey-darkest uppercase font-bold text-xl mb-3"},"Subscribe to the Newsletter"):null;return t.createElement("div",{className:"bg-white rounded-md shadow-md p-8"},l,t.createElement("p",{className:"text-sm mb-3"},"Get my latest posts and project updates by email"),t.createElement("form",{action:"https://gmail.us8.list-manage.com/subscribe?u=5adef344d2c51668e5ca2569f&id=b128317c3c",method:"post",className:"flex flex-col"},t.createElement("input",{placeholder:"First Name",type:"text",name:"FNAME",className:n,required:!0}),t.createElement("input",{placeholder:"Email",type:"email",name:"EMAIL",className:n,required:!0}),t.createElement("div",{className:"hidden","aria-hidden":"true"},t.createElement("input",{type:"text",name:"b_7714f14ff32085c685da2cfaa_53ffa81463",tabIndex:-1})),t.createElement("input",{type:"submit",value:"Subscribe",className:"transition duration-200 ease-in-out bg-black text-white py-2 px-3 rounded shadow-sm cursor-pointer hover:bg-gray-800"})))}},2717:function(e,a,n){"use strict";n.d(a,{j:function(){return t}});const t=e=>{if("number"!=typeof e)return null;return Math.ceil(1*e)||1}},1151:function(e,a,n){"use strict";n.d(a,{ah:function(){return s}});var t=n(7294);const l=t.createContext({});function s(e){const a=t.useContext(l);return t.useMemo((()=>"function"==typeof e?e(a):{...a,...e}),[a,e])}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-posts-2024-deploying-large-language-models-llms-to-production-self-hosted-on-gcp-index-mdx-089bbfe3ffba3aeebdce.js.map