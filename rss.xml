<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Hao Hoang RSS Feed]]></title><description><![CDATA[Personal blog and projects by Van Hao Hoang]]></description><link>https://vanhaohoang.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Sat, 14 Sep 2024 15:10:34 GMT</lastBuildDate><item><title><![CDATA[Efficient LLM Inference with Limited Memory by Apple]]></title><description><![CDATA[Apple's innovative approach to running large language models locally, using flash memory and dynamic parameter loading to exceed DRAM limitations.]]></description><link>https://vanhaohoang.github.io/blog/2024/efficient-llm-inference-with-limited-memory-by-apple/</link><guid isPermaLink="false">https://vanhaohoang.github.io/blog/2024/efficient-llm-inference-with-limited-memory-by-apple/</guid><pubDate>Sat, 14 Sep 2024 00:00:00 GMT</pubDate><content:encoded>Apple&apos;s innovative approach to running large language models locally, using flash memory and dynamic parameter loading to exceed DRAM limitations.</content:encoded></item></channel></rss>