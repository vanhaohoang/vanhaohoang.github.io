{"componentChunkName":"component---src-templates-post-tsx-content-file-path-src-posts-2024-efficient-llm-inference-with-limited-memory-by-apple-index-mdx","path":"/blog/2024/efficient-llm-inference-with-limited-memory-by-apple/","result":{"data":{"mdx":{"id":"299d690f-b499-50fe-bdf9-80ee4c542505","body":"\nApple has found a way to run large language models (LLMs) locally, exceeding available DRAM capacity by leveraging flash memory for LLM weight tensor storage and dynamic loading into DRAM.\n\n- [ARXIV preprint by Apple](https://lnkd.in/gxASDYJX) titled \"Efficient Large Language Model Inference with Limited Memory\" introduces this innovative technique.\n- By dynamically loading parameters from flash memory, Apple addresses the challenge of running models that are too large for DRAM alone.\n- This method involves exploiting sparsity in the FeedForward Network (FFN) layers, using selective loading of parameters, sparsity prediction, and static memory preallocation.\n\n## Key Insights\n\n- Standard methods require loading the entire model into DRAM, but Apple's approach selectively loads parameters from flash memory as needed.\n- The model utilizes the HuggingFace transformers, KV caching, and a 32-thread reading process to optimize performance.\n\n> See the full paper on [ARXIV](https://lnkd.in/gxASDYJX).\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nI3uYT3quxQ?si=TRD-2-tpR1T7Wb-n\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Memory Allocation Breakdown\n\nThe memory retained in DRAM for Apple's model includes multiple components:\n\n- **Embeddings**: 3% of the model size.\n- **Attention Model weights**: 32.3%.\n- **Feed Forward Network (FFN)**: 15.5% (calculated as 0.24 × 64.62).\n- **Predictor**: 1.25%.\n\nThe total DRAM usage amounts to 52.1% of the model size.\n\n> Hint: The Mixture of Experts (MoE) model, which has sparse structures in its FFN layer, could combine well with Apple’s method, allowing even larger MoEs to run on personal devices.\n\nSource: [Apple’s ARXIV paper](https://lnkd.in/gXtXtubx)\n\n## Deep Dive:\n\n- [Full ARXIV Preprint](https://lnkd.in/gxASDYJX)","fields":{"slug":"/blog/2024/efficient-llm-inference-with-limited-memory-by-apple/"},"internal":{"contentFilePath":"/Users/haohoang/Desktop/Learning/vanhaohoang.github.io/src/posts/2024/efficient-llm-inference-with-limited-memory-by-apple/index.mdx"},"frontmatter":{"title":"Efficient LLM Inference with Limited Memory by Apple","summary":"Apple's innovative approach to running large language models locally, using flash memory and dynamic parameter loading to exceed DRAM limitations.","date":"14 September, 2024","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#484858","images":{"fallback":{"src":"/static/206bb7b07ec9708aa019f377efa051c1/77b4b/efficient-llms.jpg","srcSet":"/static/206bb7b07ec9708aa019f377efa051c1/58451/efficient-llms.jpg 750w,\n/static/206bb7b07ec9708aa019f377efa051c1/f39cf/efficient-llms.jpg 1080w,\n/static/206bb7b07ec9708aa019f377efa051c1/77b4b/efficient-llms.jpg 1280w","sizes":"100vw"},"sources":[{"srcSet":"/static/206bb7b07ec9708aa019f377efa051c1/ee401/efficient-llms.webp 750w,\n/static/206bb7b07ec9708aa019f377efa051c1/de1b1/efficient-llms.webp 1080w,\n/static/206bb7b07ec9708aa019f377efa051c1/92bb1/efficient-llms.webp 1280w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5625}}}}}},"pageContext":{"slug":"/blog/2024/efficient-llm-inference-with-limited-memory-by-apple/","frontmatter":{"title":"Efficient LLM Inference with Limited Memory by Apple","summary":"Apple's innovative approach to running large language models locally, using flash memory and dynamic parameter loading to exceed DRAM limitations.","cover":"assets/efficient-llms.jpg","date":"2024-09-14T00:00:00.000Z"}}},"staticQueryHashes":[],"slicesMap":{}}