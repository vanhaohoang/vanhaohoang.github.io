{"componentChunkName":"component---src-templates-post-tsx-content-file-path-src-posts-2024-deploying-large-language-models-llms-to-production-self-hosted-on-gcp-index-mdx","path":"/blog/2024/deploying-large-language-models-llms-to-production-self-hosted-on-gcp/","result":{"data":{"mdx":{"id":"bf8d2e1c-ba71-5a2e-9e5a-373e01100ffb","body":"\n## Objective\n\nThis document outlines the various open-source libraries, tools, and best practices for deploying Large Language Models (LLMs) in a self-hosted production environment on Google Cloud Platform (GCP). The primary focus is on ensuring high security by avoiding the use of third-party services.\n\n## Compare Open-Source Tools for Model Serving\n\n<table style={{ borderCollapse: 'collapse', width: '100%' }}>\n  <thead>\n    <tr>\n      <th style={{ border: '1px solid black', padding: '8px' }}>Tool</th>\n      <th style={{ border: '1px solid black', padding: '8px' }}>Focus</th>\n      <th style={{ border: '1px solid black', padding: '8px' }}>Model Support</th>\n      <th style={{ border: '1px solid black', padding: '8px' }}>Usage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style={{ border: '1px solid black', padding: '8px' }}>vLLM</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>GPU</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Limited Models</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>GPU Server</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Llama.cpp</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>CPU</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Any Models</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>CPU Server</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Llamafile</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>CPU</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Any Models</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>CPU Server</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Ollama</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>CPU</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Any Models</td>\n      <td style={{ border: '1px solid black', padding: '8px' }}>Developer (Build/Test)</td>\n    </tr>\n  </tbody>\n</table>\n\n## Deploy LLMs Model using vLLM\n\n### System Requirements\n\n- GPU accelerator for fast inference latency\n- GPU with large memory (e.g., NVIDIA A100)\n- Use a system with resources matching the model's fine-tuning requirements\n- Using vLLM (version `0.6.0`), providing an [OpenAI compatible server](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n\n### Deploy the Fine-Tuned Model to a Development or Local Environment\n\nAssuming you have:\n\n- [vLLM==0.6.0](https://docs.vllm.ai/en/latest/getting_started/installation.html)\n- Fine-tuned model files in a directory called `finetuned_model`\n\nYou can deploy it locally using the following command:\n\n```bash\nvllm serve finetuned_model\n```\n\n> **Note:**\n> - The default model name for API calls will be `finetuned_model` unless overridden with flags.\n> - Access the vLLM server at `localhost`.\n> - Refer to the [official documentation](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server) for available flags.\n\n### Deploy the Fine-Tuned Model to a Production Environment\n\n#### VM Instance\n\n- Use the [official vLLM Docker image](https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html) for running a fine-tuned model on a VM instance (vLLM `0.6.0`).\n- Prerequisites:\n  - [Docker](https://docs.docker.com/engine/install/)\n  - [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\n  - Fine-tuned model files in `finetuned_model`\n\nRun the following Docker command, updating the `--served-model-name` flag to override the model name:\n\n```docker\ndocker run --runtime nvidia --gpus all \\\n  -v \"./finetuned_model:/mnt/models\" \\\n  -p 8000:8000 --ipc=\"host\" \\\n  vllm/vllm-openai:latest \\\n  --model \"/mnt/models\" \\\n  --served-model-name \"CHANGEME\"\n```\n\n#### Kubernetes Cluster\n\nWe recommend using [KServe](https://github.com/kserve/kserve) when deploying a fine-tuned model to a [Kubernetes](https://kubernetes.io/) cluster, due to the [integrated vLLM runtime](https://kserve.github.io/website/latest/modelserving/v1beta1/llm/huggingface/).\n\nEnsure you have:\n\n- A Kubernetes cluster with [KServe installed](https://kserve.github.io/website/latest/admin/serverless/serverless/)\n- At least one [NVIDIA GPU node with appropriate drivers](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)\n\nUse the following manifest to deploy the model from a cloud storage bucket:\n\n```yaml\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: \"CHANGEME\"\nspec:\n  predictor:\n    containers:\n    - name: \"main\"\n      image: \"kserve/vllmserver:latest\"\n      command:\n      - \"python3\"\n      - \"-m\"\n      - \"vllm.entrypoints.openai.api_server\"\n      args:\n      - \"--port\"\n      - \"8000\"\n      - \"--model\"\n      - \"/mnt/models\"\n      - \"--served-model-name\"\n      - \"CHANGEME\"\n      env:\n      - name: \"STORAGE_URI\"\n        value: \"CHANGEME\"\n      resources:\n        limits:\n          nvidia.com/gpu: \"1\"\n```\n\nMake sure to update the values for:\n\n- `metadata.name` for your model inference service\n- `STORAGE_URI` for your cloud storage URI\n- `--served-model-name` for the model name used in API calls\n\n## Deploy LLMs Model using Ollama\n\n### Deploy the Fine-Tuned Model to a Development or Local Environment\n\n### Deploy the Fine-Tuned Model to a Production Environment\n\nUse the following Dockerfile to deploy Ollama to production:\n\n```docker\nFROM ollama/ollama:0.3.6\n\nENV OLLAMA_HOST 0.0.0.0:8080\nENV OLLAMA_MODELS /models\nENV OLLAMA_DEBUG false\nENV OLLAMA_KEEP_ALIVE -1\nENV MODEL gemma2:9b\n\nRUN ollama serve & sleep 5 && ollama pull $MODEL \n\nENTRYPOINT [\"ollama\", \"serve\"]\n```\n\nCreate an artifact repository on GCP:\n\n```bash\ngcloud artifacts repositories create ollama \\\n  --repository-format=docker \\\n  --location=us-central1\n```\n\nBuild the image and submit it:\n\n```bash\ngcloud builds submit \\\n  --tag us-central1-docker.pkg.dev/[PROJECT-NAME]/ollama/ollama-gemma \\\n  --machine-type e2-highcpu-32\n```\n\nCreate a service account:\n\n```bash\ngcloud iam service-accounts create ollama \\\n  --display-name=\"Service Account for Ollama Cloud Run service\"\n```\n\nActivate the model:\n\n```bash\ngcloud beta run deploy ollama-gemma \\\n  --image us-central1-docker.pkg.dev/[PROJECT-NAME]/ollama/ollama-gemma \\\n  --concurrency 4 \\\n  --cpu 8 \\\n  --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n  --max-instances 2 \\\n  --memory 32Gi \\\n  --no-allow-unauthenticated \\\n  --no-cpu-throttling \\\n  --service-account ollama@[PROJECT-NAME].iam.gserviceaccount.com \\\n  --timeout=600\n```\n\nThe model can now be accessed using `curl`:\n\n```bash\ncurl -H \"Authorization: Bearer $(gcloud auth print-identity-token)\" \\\n     -H \"Content-Type: application/json\" \\\n     https://ollama-gemma-wruklsvs3a-uc.a.run.app/api/generate -d '{\n       \"model\": \"gemma2:9b\",\n       \"prompt\": \"Why is the sky blue?\",\n       \"stream\": false\n     }'\n```\n\n## Deploy LLMs Model using Llamafile\n\n### Step 1: Download the Llamafile\n\nDownload the `llava-v1.5-7b-q4.llamafile` (3.97 GB) executable from [here](https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true).\n\n### Step 2: Grant Execution Permission\n\nFor macOS, Linux, or BSD users:\n\n```bash\nchmod +x llava-v1.5-7b-q4.llamafile\n```\n\n### Step 3: Run the Llamafile\n\n```bash\n./llava-v1.5-7b-q4.llamafile -ngl 9999\n```\n\n### Step 4: Interact with the UI\n\nAfter running, the Llamafile will open the user interface at [http://localhost:8080](http://localhost:8080/).\n\n![Llamafile UI](assets/llamafile-ui.png)\n","fields":{"slug":"/blog/2024/deploying-large-language-models-llms-to-production-self-hosted-on-gcp/"},"internal":{"contentFilePath":"/Users/haohoang/Desktop/Learning/vanhaohoang.github.io/src/posts/2024/deploying-large-language-models-llms-to-production-self-hosted-on-gcp/index.mdx"},"frontmatter":{"title":"Deploying Large Language Models (LLMs) to Production (Self-Hosted on GCP)","summary":"Best practices, tools, and guidelines for deploying LLMs in a production environment using open-source tools on Google Cloud Platform (GCP). Focus on maintaining security by avoiding external services.","date":"20 September, 2024","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#080818","images":{"fallback":{"src":"/static/1e517ef81a6116b9e35792cbb0145194/23f3d/deploying-llms-gcp.jpg","srcSet":"/static/1e517ef81a6116b9e35792cbb0145194/1fe0b/deploying-llms-gcp.jpg 750w,\n/static/1e517ef81a6116b9e35792cbb0145194/ac202/deploying-llms-gcp.jpg 1080w,\n/static/1e517ef81a6116b9e35792cbb0145194/1ad53/deploying-llms-gcp.jpg 1366w,\n/static/1e517ef81a6116b9e35792cbb0145194/23f3d/deploying-llms-gcp.jpg 1418w","sizes":"100vw"},"sources":[{"srcSet":"/static/1e517ef81a6116b9e35792cbb0145194/15499/deploying-llms-gcp.webp 750w,\n/static/1e517ef81a6116b9e35792cbb0145194/e14f7/deploying-llms-gcp.webp 1080w,\n/static/1e517ef81a6116b9e35792cbb0145194/6d0b2/deploying-llms-gcp.webp 1366w,\n/static/1e517ef81a6116b9e35792cbb0145194/6701a/deploying-llms-gcp.webp 1418w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5176304654442877}}}}}},"pageContext":{"slug":"/blog/2024/deploying-large-language-models-llms-to-production-self-hosted-on-gcp/","frontmatter":{"title":"Deploying Large Language Models (LLMs) to Production (Self-Hosted on GCP)","summary":"Best practices, tools, and guidelines for deploying LLMs in a production environment using open-source tools on Google Cloud Platform (GCP). Focus on maintaining security by avoiding external services.","cover":"assets/deploying-llms-gcp.jpg","date":"2024-09-20T00:00:00.000Z"}}},"staticQueryHashes":[],"slicesMap":{}}